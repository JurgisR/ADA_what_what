{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yev/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import folium\n",
    "import datetime, time\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.stats.api as sms\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "plt.rcParams['figure.figsize'] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics\n",
    "\n",
    "def descr_stats(df):\n",
    "    df.describe()\n",
    "    \n",
    "def correlation(x, y, corr = 'spearman'):\n",
    "    \n",
    "    if corr == 'spearman':\n",
    "        return stats.spearmanr(x,y)\n",
    "    if corr == 'pearson':\n",
    "        return stats.pearsonr(x,y)\n",
    "    \n",
    "def t_test(df1, df2):\n",
    "    return ttest_ind(df1, df2)\n",
    "    \n",
    "def conf_interval_95(data):\n",
    "    # confidence interval for 95% confidence equals mean +- (1.96*std /sqrt(sample_size))\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    n = len(data)\n",
    "    interval = np.array([mean - (1.96 * std/np.sqrt(n)), mean + (1.96 * std/np.sqrt(n))])\n",
    "    return interval\n",
    "\n",
    "def LMplot (set, xaxis, yaxis, degree):\n",
    "    '''\n",
    "    Function fits linear regression model and calculates corresponding statistics.\n",
    "    Data and regression line is then plotted\n",
    "     :input: pandas dataframe with columns\n",
    "     :output: print statistics and plot linear regression\n",
    "     :return: -\n",
    "     '''\n",
    "    x= set[xaxis]\n",
    "    y= set[yaxis]\n",
    "\n",
    "    #calculate linear regression with polynomials of specified degree\n",
    "    fit = np.polyfit(x, y, degree)\n",
    "\n",
    "    #plot model and existing data\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.plot(x, y, 'o',ms=2, label='original data')\n",
    "    plt.plot(np.sort(x),np.polyval(fit,np.sort(x)),'-r', label='fitted line')\n",
    "    plt.legend()\n",
    "    plt.xlabel(xaxis)\n",
    "    plt.ylabel(yaxis)\n",
    "    plt.show()\n",
    "    \n",
    "def LMplot (set, xaxis, yaxis,):\n",
    "    '''\n",
    "    Function fits linear regression model and calculates corresponding statistics.\n",
    "    Data and regression line is then plotted\n",
    "     :input: pandas dataframe with columns named 'SelfEmployed' and 'IncomePerCap'\n",
    "     :output: print statistics and plot linear regression\n",
    "     :return: -\n",
    "     '''\n",
    "    \n",
    "    x= set[xaxis]\n",
    "    y= set[yaxis]\n",
    "\n",
    "    #calculate linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "\n",
    "    #plot model and existing data\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.plot(x, y, 'o',ms=2, label='original data')\n",
    "    plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
    "    plt.legend()\n",
    "    plt.xlabel(xaxis)\n",
    "    plt.ylabel(yaxis)\n",
    "    plt.show()\n",
    "\n",
    "    #print statistics\n",
    "    print('''The formula for the fitted linear Model is: y = %f + %f * x \\n \\n r-value = %f\n",
    " p-value = %f \\n Standard_error: %f''' % (intercept, slope,r_value, p_value, std_err)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning\n",
    "\n",
    "def make_binary(df, column, key):\n",
    "    df[column] = df[column].map(lambda x: 1 if x==key else 0)\n",
    "    return df\n",
    "\n",
    "def convert_categorical(df):\n",
    "    df = pd.get_dummies(df)\n",
    "    return df\n",
    "\n",
    "def array_lambda():\n",
    "    #prediction for class = 1\n",
    "    prediction = np.asarray([1 if pred[1] >= 0.5 else 0 for pred in prediction])\n",
    "\n",
    "def standardize(x):\n",
    "    #standardize data\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def split_train_data(data, labels, fraction):\n",
    "    #Split data into training and test set\n",
    "    X_train_df = data.sample(frac=fraction,random_state=100)\n",
    "    X_test_df = data.drop(X_train_df.index)\n",
    "    \n",
    "    #Retrieve the lables based on the indeces of the X_train and X_test\n",
    "    y_train = labels.iloc[list(X_train_df.index),].values\n",
    "    y_test = labels.iloc[list(X_test_df.index),].values\n",
    "    \n",
    "    #Get only values of the data set\n",
    "    X_train = X_train_df.values\n",
    "    X_test = X_test_df.values\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def confusion_mtx(y_test, y_pred, class_):\n",
    "    #Get confusion matrix\n",
    "    conf_mtx = pd.crosstab(y_test, y_pred)\n",
    "    print(conf_mtx)\n",
    "    \n",
    "    #Extract the values from the confusion matrix\n",
    "    true_pos = conf_matrix[1][1]\n",
    "    true_neg = conf_matrix[0][0]\n",
    "    false_pos = conf_matrix[0][1]\n",
    "    false_neg = conf_matrix[1][0]\n",
    "    \n",
    "    #Manually compute the different performance parameters\n",
    "    accuracy = (true_pos + true_neg) / conf_mtx.values.sum()\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg) \n",
    "    fscore = 2*precision*recall / (precision + recall)\n",
    "    \n",
    "    print('For Class {}:'.format(class_),'\\n','Accuracy is {}'.format(accuracy),'\\n','Precision is {}'.format(precision),'\\n',\n",
    "             'Recall is {}'.format(recall),'\\n','F1-score is {}'.format(fscore),'\\n','\\n')\n",
    "\n",
    "def Logistic_reg(X_train, y_train, X_test):\n",
    "    lr = linear_model.LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    prediction = lr.predict_proba(X_test)\n",
    "    #prediction for class = 1 with threshold 0.5\n",
    "    prediction_one = np.asarray([1 if pred[1] >= 0.5 else 0 for pred in prediction])\n",
    "    return prediction_one\n",
    "\n",
    "def mask_array_xtest(X_test, selector):\n",
    "    X_test_k = np.asarray([row[selector.get_support()] for row in X_test])\n",
    "    return X_test_k\n",
    "\n",
    "def get_features(df, selector):\n",
    "    features = df.columns[selector.get_support()]\n",
    "    return features\n",
    "\n",
    "def RandomForest_GridSearch(X_train, y_train, cv=5):\n",
    "    rfc = RandomForestClassifier()\n",
    "\n",
    "    param_grid = { \n",
    "        'n_estimators': np.linspace(1, 100, 20, endpoint=True, dtype=int),\n",
    "        'max_depth': np.linspace(1, 20, 10, endpoint=True, dtype=int)\n",
    "        #'max_features': np.linspace(1, 50, 25, endpoint=True, dtype=int)\n",
    "        }\n",
    "\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= cv)\n",
    "    CV_rfc.fit(X_train, y_train)\n",
    "    print(CV_rfc.best_params_)\n",
    "\n",
    "def RandomForest(X_train, y_train, X_test, estimators, depth):\n",
    "    rfc = RandomForestClassifier(\n",
    "            n_estimators= estimators, \n",
    "            max_depth=depth\n",
    "            )\n",
    "    rfc.fit(X_train, y_train)\n",
    "    #feature importance\n",
    "    feature_importance = rfc.feature_importances_\n",
    "    #prediction\n",
    "    prediction = rfc.predict(X_test)\n",
    "    return prediction\n",
    "\n",
    "#Cross Validation\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    Function that builds k random indices from the length of the\n",
    "    dataset for the cross validation\n",
    "    Returns\n",
    "       - random indeces from our dataset\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, estim, depth):\n",
    "    '''\n",
    "    Does the k-fold (5) split on the dataset and performs cross validation\n",
    "    '''\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te_c = y[te_indice]\n",
    "    y_tr_c = y[tr_indice]\n",
    "    x_te_c = x[te_indice]\n",
    "    x_tr_c = x[tr_indice]\n",
    "    \n",
    "    forest = RandomForestClassifier(n_estimators=estim,\n",
    "                            max_depth=depth).fit(x_tr, y_tr)\n",
    "    y_tester = forest.predict(x_te)\n",
    "    f1 = f1_score(y_te, y_tester)\n",
    "    return f1\n",
    "    \n",
    "\n",
    "\n",
    "# Performs the cross validation testing on the data   \n",
    "def cross_val_test(y, x):\n",
    "    seed = 1\n",
    "    k_fold=5\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    f1_kfold = []\n",
    "    f1_full = []\n",
    "    estim = [10,25,50,100]\n",
    "    depth = [2,4,10]\n",
    "    for esti in estim:\n",
    "        for dept in depth:  \n",
    "            for i in range(k_fold):\n",
    "                f1 = cross_validation(y, x, k_indices, i, esti, dept)\n",
    "                f1_kfold.append(f1)\n",
    "            f1_full.append(np.sum(f1_kfold)/k_fold)\n",
    "            print('For {} n_estimators and {} max_depth we get an F1 score of {},'.format(esti,dept, np.sum(f1_kfold)/k_fold))\n",
    "            f1_kfold = []\n",
    "    return f1_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
